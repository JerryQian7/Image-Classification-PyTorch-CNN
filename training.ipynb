{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import model\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import custom_model\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the GPU. The following line is just a check to see if GPU is availables\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "root = 'birds_dataset/images/'\n",
    "\n",
    "# Small datasets used for local testing\n",
    "# train_dataset = datasets.bird_dataset(root, 'birds_dataset/small_train_list.txt')\n",
    "# test_dataset = datasets.bird_dataset(root, 'birds_dataset/small_test_list.txt')\n",
    "\n",
    "# Full datasets\n",
    "train_dataset = datasets.bird_dataset(root, 'birds_dataset/train_list.txt')\n",
    "test_dataset = datasets.bird_dataset(root, 'birds_dataset/test_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off validation sets\n",
    "num_classes = 20\n",
    "val_imgs_per_class = 3\n",
    "\n",
    "# Define validation indices\n",
    "val1_idxs = list(range(0, len(train_dataset), \n",
    "                       int(len(train_dataset) / num_classes / val_imgs_per_class)))\n",
    "train1_idxs = list(set(list(range(len(train_dataset)))) - set(val1_idxs))\n",
    "val2_idxs = list(range(val_imgs_per_class, len(train_dataset), \n",
    "                       int(len(train_dataset) / num_classes / val_imgs_per_class)))\n",
    "train2_idxs = list(set(list(range(len(train_dataset)))) - set(val2_idxs))\n",
    "\n",
    "# Define validation subsets\n",
    "val1_dataset = torch.utils.data.Subset(train_dataset, val1_idxs)\n",
    "train1_dataset = torch.utils.data.Subset(train_dataset, train1_idxs)\n",
    "val2_dataset = torch.utils.data.Subset(train_dataset, val2_idxs)\n",
    "train2_dataset = torch.utils.data.Subset(train_dataset, train2_idxs)\n",
    "\n",
    "# Define dataloaders\n",
    "train1_dataloader = DataLoader(train1_dataset, batch_size=5, shuffle=True, num_workers=2)\n",
    "val1_dataloader = DataLoader(val1_dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "\n",
    "train2_dataloader = DataLoader(train2_dataset, batch_size=5, shuffle=True, num_workers=2)\n",
    "val2_dataloader = DataLoader(val2_dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization functions\n",
    "# Xavier initialization\n",
    "def init_weights_xavier(model):\n",
    "    if type(model) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(model.weight)\n",
    "        model.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over batches within a single epoch\n",
    "# Calculate and return loss, accuracy\n",
    "def batch_loop(model, criterion, optimizer, step, step_gamma, dataloader, training=True):\n",
    "    \"\"\"\n",
    "    model      - the neural network model being trained or evaluated\n",
    "    criterion  - used to calculate the loss\n",
    "    optimizer  - used to optimize during training\n",
    "    step       - step size for learning rate decay. None if no decay is used\n",
    "    step_gamma - gamma for learning rate decay. None if no decay is used\n",
    "    dataloader - the dataset on which model is being trained or evaluated\n",
    "    training   - determines whether model is being trained (True) or evaluated (False)\n",
    "    \"\"\"\n",
    "    model.training = training\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "\n",
    "    # Step-wise learning rate decay\n",
    "    scheduler = None\n",
    "    if step is not None:\n",
    "        scheduler = StepLR(optimizer, step_size=step, gamma=step_gamma)\n",
    "\n",
    "    num_batches = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Ensure parameter gradients start at zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Calculate loss & backprop only if in train mode\n",
    "        loss = criterion(output, labels.long())\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Record running loss & accuracy\n",
    "        running_loss += loss.item() # Current loss is average over batch\n",
    "        preds = torch.argmax(output, axis=1)\n",
    "        running_acc += (preds == labels).float().sum().item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    if training and step is not None:\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Return average loss and accuracy over size of dataset\n",
    "    return (running_loss / num_batches), (running_acc / len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains model on train_dataloader dataset for num_epochs; evaluates on val_dataloader for each epoch.\n",
    "# Returns best model & its validation loss, as well as loss & accuracy values across epochs\n",
    "def train_loop(model, criterion, optimizer, step, step_gamma, train_dataloader, val_dataloader, num_epochs):\n",
    "    \"\"\"\n",
    "    model            - the neural network model being trained\n",
    "    criterion        - used to calculate the loss\n",
    "    optimizer        - used to optimize during training\n",
    "    step             - step size for learning rate decay. None if no decay is used\n",
    "    step_gamma       - gamma for learning rate decay. None if no decay is used\n",
    "    train_dataloader - the dataset on which to train the model\n",
    "    val_dataloader   - the dataset on which to evaluate the model\n",
    "    num_epochs       - number of training epochs\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    best_model = model\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = \\\n",
    "            batch_loop(model, criterion, optimizer, step, step_gamma, train_dataloader, training=True)\n",
    "        val_loss, val_accuracy = \\\n",
    "            batch_loop(model, criterion, optimizer, step, step_gamma, val_dataloader, training=False)\n",
    "\n",
    "        # Record epoch loss, accuracy\n",
    "        print(\"Epoch\", epoch, \"train loss, acc:\", train_loss, train_accuracy)\n",
    "        print(\"Epoch\", epoch, \"val loss, acc:\", val_loss, val_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "        # Record best model\n",
    "        if val_loss < best_loss:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_loss = val_loss\n",
    "\n",
    "    print(\"Best:\", best_loss)\n",
    "    return best_model, best_loss, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with 2-fold cross validation, using pre-loaded training & validation sets.\n",
    "# Reinitializes the model between epochs to ensure a clean slate.\n",
    "\n",
    "# For each epoch iterate over your dataloaders/datasets, pass it to your NN model, get output,\n",
    "# calculate loss, and backpropagate using optimizer\n",
    "\n",
    "# Returns final models, optimizers, criterion, as well as per-epoch loss & accuracy\n",
    "def train_validation(epochs, model_init, hyperparam_init, step, step_gamma):\n",
    "    \"\"\"\n",
    "    epochs          - number of training epochs\n",
    "    model_init      - initializes and returns the model\n",
    "    hyperparam_init - initializes and returns the model's criterion and optimizer\n",
    "    step            - step size for learning rate decay. None if no decay is used\n",
    "    step_gamma      - gamma for learning rate decay. None if no decay is used\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation set 1\n",
    "    # (Re)initialize model before each training loop\n",
    "    model = model_init()\n",
    "    criterion, optimizer_1 = hyperparam_init(model)\n",
    "    best_model_1, best_loss_1, train_losses_1, train_accs_1, val_losses_1, val_accs_1 = \\\n",
    "        train_loop(model, criterion, optimizer_1, step, step_gamma, \n",
    "                   train1_dataloader, val1_dataloader, num_epochs=epochs)\n",
    "\n",
    "    # Validation set 2\n",
    "    model = model_init()\n",
    "    _, optimizer_2 = hyperparam_init(model)\n",
    "    best_model_2, best_loss_2, train_losses_2, train_accs_2, val_losses_2, val_accs_2 = \\\n",
    "        train_loop(model, criterion, optimizer_2, step, step_gamma, \n",
    "                   train2_dataloader, val2_dataloader, num_epochs=epochs)\n",
    "    \n",
    "    \n",
    "    # Record average losses and accuracies across folds\n",
    "    train_losses = (np.array(train_losses_1) + np.array(train_losses_2)) / 2\n",
    "    train_accs = (np.array(train_accs_1) + np.array(train_accs_2)) / 2\n",
    "    val_losses = (np.array(val_losses_1) + np.array(val_losses_2)) / 2\n",
    "    val_accs = (np.array(val_accs_1) + np.array(val_accs_2)) / 2\n",
    "    \n",
    "    # Return best models/losses/optimizers, per-epoch loss & accuracy\n",
    "    optimizers = [optimizer_1, optimizer_2]\n",
    "    best_models = [best_model_1, best_model_2]\n",
    "    best_losses = [best_loss_1, best_loss_2]\n",
    "    losses_and_accs = [train_losses, train_accs, val_losses, val_accs]\n",
    "    return best_models, best_losses, optimizers, criterion, losses_and_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing & Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model, given two models and their validation losses\n",
    "# Return the best model & its optimizer\n",
    "def best_model(models, losses, optimizers):\n",
    "    if losses[0] <= losses[1]:\n",
    "        return models[0], optimizers[0]\n",
    "    return models[1], optimizers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the given model\n",
    "def test_model(best_model, criterion, optimizer, step, step_gamma):\n",
    "    test_loss, test_acc = batch_loop(best_model, criterion, optimizer, step, step_gamma, \n",
    "                                     test_dataloader, training=False)\n",
    "    print(\"Test loss:\", test_loss)\n",
    "    print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility plotting function\n",
    "def plot_results(train_results, val_results, num_epochs, model_type, result_type):\n",
    "    \"\"\"\n",
    "    Plot results across epochs.\n",
    "    \"\"\"\n",
    "    fig = plt.figure() \n",
    "    x = list(range(num_epochs))\n",
    "\n",
    "    plt.plot(x, train_results, label=('Training ' + result_type))\n",
    "    plt.plot(x, val_results, label=('Validation ' + result_type))\n",
    "\n",
    "    plt.legend(loc ='upper right') \n",
    "    plt.title(model_type + ': Training and Validation ' + result_type) \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(result_type)\n",
    "    plt.savefig(model_type + \"_\" + result_type + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Create NN model object\n",
    "def init_model_baseline():\n",
    "    model_baseline = model.baseline_Net(classes = 20)\n",
    "    model_baseline.to(device)\n",
    "    return model_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Initialize hyperparameters for baseline model\n",
    "def init_hyperparameters_baseline(model):\n",
    "    criterion_baseline = nn.CrossEntropyLoss()\n",
    "    optimizer_baseline = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model.apply(init_weights_xavier)\n",
    "    \n",
    "    return criterion_baseline, optimizer_baseline\n",
    "\n",
    "baseline_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train baseline model\n",
    "base_models, base_losses, base_optimizers, base_criterion, base_losses_and_accs = \\\n",
    "    train_validation(baseline_epochs, init_model_baseline, init_hyperparameters_baseline, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline model per-epoch losses & performance\n",
    "base_train_losses = base_losses_and_accs[0]\n",
    "base_train_accs = base_losses_and_accs[1]\n",
    "base_val_losses = base_losses_and_accs[2]\n",
    "base_val_accs = base_losses_and_accs[3]\n",
    "\n",
    "plot_results(base_train_losses, base_val_losses, baseline_epochs, model_type=\"Baseline\", result_type=\"Loss\")\n",
    "plot_results(base_train_accs, base_val_accs, baseline_epochs, model_type=\"Baseline\", result_type=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & test best model\n",
    "baseline_nn, baseline_optimizer = \\\n",
    "    best_model(base_models, base_losses, base_optimizers)\n",
    "test_model(baseline_nn, base_criterion, baseline_optimizer, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model\n",
    "##### Hyperparameters\n",
    "- Epochs: 100\n",
    "- Learning rate: 1e-4\n",
    "- Learning rate decay steps: 1\n",
    "- Learning rate decay gamma: 0.1\n",
    "- Batch size: 5\n",
    "- Activations: ReLU\n",
    "- Optimizer: Adam\n",
    "- Weight initialization: Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Define custom model object\n",
    "def init_model_custom():\n",
    "    model = custom_model.custom_Net(classes = 20)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Initialize hyperparameters for custom model\n",
    "def init_hyperparameters_custom(model):\n",
    "    criterion_custom = nn.CrossEntropyLoss()\n",
    "    optimizer_custom = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model.apply(init_weights_xavier)\n",
    "    \n",
    "    return criterion_custom, optimizer_custom\n",
    "\n",
    "custom_epochs = 100\n",
    "custom_step = 1\n",
    "custom_step_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train custom model\n",
    "custom_models, custom_losses, custom_optimizers, custom_criterion, custom_losses_and_accs = \\\n",
    "    train_validation(custom_epochs, init_model_custom, init_hyperparameters_custom, custom_step, \n",
    "                     custom_step_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom model per-epoch losses & performance\n",
    "custom_train_losses = custom_losses_and_accs[0]\n",
    "custom_train_accs = custom_losses_and_accs[1]\n",
    "custom_val_losses = custom_losses_and_accs[2]\n",
    "custom_val_accs = custom_losses_and_accs[3]\n",
    "\n",
    "plot_results(custom_train_losses, custom_val_losses, custom_epochs, model_type=\"Custom\", result_type=\"Loss\")\n",
    "plot_results(custom_train_accs, custom_val_accs, custom_epochs, model_type=\"Custom\", result_type=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & test best model\n",
    "custom_nn, custom_optimizer = \\\n",
    "    best_model(custom_models, custom_losses, custom_optimizers)\n",
    "test_model(custom_nn, custom_criterion, custom_optimizer, custom_step, custom_step_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Models\n",
    "### VGG-16: Pre-Trained Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pretrained vgg-16 model\n",
    "transfer_model_vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16: Fixed Feature Extractor\n",
    "##### Hyperparameters\n",
    "- Epochs: 100\n",
    "- Learning rate: 1e-4\n",
    "- Learning rate decay steps: 1\n",
    "- Learning rate decay gamma: 0.1\n",
    "- Batch size: 5\n",
    "- Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize frozen vgg model, with last fc layer replaced\n",
    "def init_model_vgg_fixed():\n",
    "    model_vgg = copy.deepcopy(transfer_model_vgg)\n",
    "    \n",
    "    # Freeze existing parameters\n",
    "    for param in model_vgg.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_vgg.classifier[6] = torch.nn.Linear(model_vgg.classifier[6].in_features, num_classes)\n",
    "    model_vgg.to(device)\n",
    "    return model_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters for fixed vgg model\n",
    "def init_hyperparams_vgg_fixed(model_vgg):\n",
    "    criterion_vgg = nn.CrossEntropyLoss()\n",
    "    optimizer_vgg = torch.optim.Adam(model_vgg.parameters(), lr=1e-4)\n",
    "    return criterion_vgg, optimizer_vgg\n",
    "\n",
    "vgg_fixed_epochs = 100\n",
    "vgg_fixed_step = 1\n",
    "vgg_fixed_step_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train fixed vgg model\n",
    "vgg_fixed_models, vgg_fixed_losses, vgg_fixed_optimizers, \\\n",
    "    vgg_fixed_criterion, vgg_fixed_losses_and_accs = \\\n",
    "        train_validation(vgg_fixed_epochs, init_model_vgg_fixed, init_hyperparams_vgg_fixed, \n",
    "                         vgg_fixed_step, vgg_fixed_step_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fixed vgg model per-epoch losses & performance\n",
    "vgg_fixed_train_losses = vgg_fixed_losses_and_accs[0]\n",
    "vgg_fixed_train_accs = vgg_fixed_losses_and_accs[1]\n",
    "vgg_fixed_val_losses = vgg_fixed_losses_and_accs[2]\n",
    "vgg_fixed_val_accs = vgg_fixed_losses_and_accs[3]\n",
    "\n",
    "plot_results(vgg_fixed_train_losses, vgg_fixed_val_losses, vgg_fixed_epochs, model_type=\"Custom\",\n",
    "             result_type=\"Loss\")\n",
    "plot_results(vgg_fixed_train_accs, vgg_fixed_val_accs, vgg_fixed_epochs, model_type=\"Custom\",\n",
    "             result_type=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & test best model\n",
    "vgg_fixed_nn, vgg_fixed_optimizer = \\\n",
    "    best_model(vgg_fixed_models, vgg_fixed_losses, vgg_fixed_optimizers)\n",
    "test_model(vgg_fixed_nn, vgg_fixed_criterion, vgg_fixed_optimizer, vgg_fixed_step, vgg_fixed_step_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 Finetuned Model\n",
    "##### Hyperparameters\n",
    "- Epochs: 100\n",
    "- Learning rate: 1e-5\n",
    "- Learning rate decay steps: 1\n",
    "- Learning rate decay gamma: 0.1\n",
    "- Batch size: 5\n",
    "- Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fineturned vgg model, with last fc layer replaced\n",
    "def init_model_vgg_tuned():\n",
    "    model_vgg = copy.deepcopy(transfer_model_vgg)\n",
    "    \n",
    "    model_vgg.classifier[6] = torch.nn.Linear(model_vgg.classifier[6].in_features, num_classes)\n",
    "    model_vgg.to(device)\n",
    "    return model_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters for finetuned vgg model\n",
    "def init_hyperparams_vgg_tuned(model_vgg):\n",
    "    criterion_vgg = nn.CrossEntropyLoss()\n",
    "    optimizer_vgg = torch.optim.Adam(model_vgg.parameters(), lr=1e-5)\n",
    "    return criterion_vgg, optimizer_vgg\n",
    "\n",
    "vgg_tuned_epochs = 25\n",
    "vgg_tuned_step = 1\n",
    "vgg_tuned_step_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train finetuned vgg model\n",
    "vgg_tuned_models, vgg_tuned_losses, vgg_tuned_optimizers, \\\n",
    "    vgg_tuned_criterion, vgg_tuned_losses_and_accs = \\\n",
    "        train_validation(vgg_tuned_epochs, init_model_vgg_tuned, init_hyperparams_vgg_tuned, \n",
    "                         vgg_tuned_step, vgg_tuned_step_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tuned vgg model per-epoch losses & performance\n",
    "vgg_tuned_train_losses = vgg_tuned_losses_and_accs[0]\n",
    "vgg_tuned_train_accs = vgg_tuned_losses_and_accs[1]\n",
    "vgg_tuned_val_losses = vgg_tuned_losses_and_accs[2]\n",
    "vgg_tuned_val_accs = vgg_tuned_losses_and_accs[3]\n",
    "\n",
    "plot_results(vgg_tuned_train_losses, vgg_tuned_val_losses, vgg_tuned_epochs, model_type=\"VGG-Tuned\", \\\n",
    "             result_type=\"Loss\")\n",
    "plot_results(vgg_tuned_train_accs, vgg_tuned_val_accs, vgg_tuned_epochs, model_type=\"VGG-Tuned\", \\\n",
    "             result_type=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & test best model\n",
    "vgg_tuned_nn, vgg_tuned_optimizer = \\\n",
    "    best_model(vgg_tuned_models, vgg_tuned_losses, vgg_tuned_optimizers)\n",
    "test_model(vgg_tuned_nn, vgg_tuned_criterion, vgg_tuned_optimizer, vgg_tuned_step, vgg_tuned_step_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 18: Pre-Trained Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pretrained resnet 18 model\n",
    "transfer_model_resnet = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 18: Fixed Feature Extractor\n",
    "##### Hyperparameters\n",
    "- Epochs: 100\n",
    "- Learning rate: 1e-4\n",
    "- Learning rate decay steps: 1\n",
    "- Learning rate decay gamma: 0.9\n",
    "- Batch size: 5\n",
    "- Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize frozen resnet model, with last fc layer replaced\n",
    "def init_model_resnet_fixed():\n",
    "    model_resnet = copy.deepcopy(transfer_model_resnet)\n",
    "    # Freeze existing parameters\n",
    "    for param in model_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_resnet.fc = torch.nn.Linear(model_resnet.fc.in_features, num_classes)\n",
    "    model_resnet.to(device)\n",
    "    return model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters for fixed resnet model\n",
    "def init_hyperparams_resnet_fixed(model_resnet):\n",
    "    criterion_resnet = nn.CrossEntropyLoss()\n",
    "    optimizer_resnet = torch.optim.Adam(model_resnet.parameters(), lr=1e-4)\n",
    "    return criterion_resnet, optimizer_resnet\n",
    "\n",
    "resnet_fixed_epochs = 100\n",
    "resnet_fixed_step = 1\n",
    "resnet_fixed_step_gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train fixed resnet model\n",
    "resnet_fixed_models, resnet_fixed_losses, resnet_fixed_optimizers, \\\n",
    "    resnet_fixed_criterion, resnet_fixed_losses_and_accs = \\\n",
    "        train_validation(resnet_fixed_epochs, init_model_resnet_fixed, init_hyperparams_resnet_fixed, \n",
    "                         resnet_fixed_step, resnet_fixed_step_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fixed resnet model per-epoch losses & performance\n",
    "resnet_fixed_train_losses = resnet_fixed_losses_and_accs[0]\n",
    "resnet_fixed_train_accs = resnet_fixed_losses_and_accs[1]\n",
    "resnet_fixed_val_losses = resnet_fixed_losses_and_accs[2]\n",
    "resnet_fixed_val_accs = resnet_fixed_losses_and_accs[3]\n",
    "\n",
    "plot_results(resnet_fixed_train_losses, resnet_fixed_val_losses, resnet_fixed_epochs, model_type=\"Resnet-Fixed\",\n",
    "             result_type=\"Loss\")\n",
    "plot_results(resnet_fixed_train_accs, resnet_fixed_val_accs, resnet_fixed_epochs, model_type=\"Resnet-Fixed\",\n",
    "             result_type=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & test best model\n",
    "resnet_fixed_nn, resnet_fixed_optimizer = \\\n",
    "    best_model(resnet_fixed_models, resnet_fixed_losses, resnet_fixed_optimizers)\n",
    "\n",
    "test_model(resnet_fixed_nn, resnet_fixed_criterion, resnet_fixed_optimizer, resnet_fixed_step, \n",
    "           resnet_fixed_step_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 18: Finetuned Model\n",
    "##### Hyperparameters\n",
    "- Epochs: 25\n",
    "- Learning rate: 1e-4\n",
    "- Learning rate decay steps: 1\n",
    "- Learning rate decay gamma: 0.9\n",
    "- Batch size: 5\n",
    "- Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize frozen resnet model, with last fc layer replaced\n",
    "def init_model_resnet_tuned():\n",
    "    model_resnet = copy.deepcopy(transfer_model_resnet)\n",
    "\n",
    "    model_resnet.fc = torch.nn.Linear(model_resnet.fc.in_features, num_classes)\n",
    "    model_resnet.to(device)\n",
    "    return model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters for fixed resnet model\n",
    "def init_hyperparams_resnet_tuned(model_resnet):\n",
    "    criterion_resnet = nn.CrossEntropyLoss()\n",
    "    optimizer_resnet = torch.optim.Adam(model_resnet.parameters(), lr=1e-4)\n",
    "    return criterion_resnet, optimizer_resnet\n",
    "\n",
    "resnet_tuned_epochs = 25\n",
    "resnet_tuned_step = 1\n",
    "resnet_tuned_step_gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train fixed resnet model\n",
    "resnet_tuned_models, resnet_tuned_losses, resnet_tuned_optimizers, resnet_tuned_criterion, \\\n",
    "    resnet_tuned_losses_and_accs = \\\n",
    "        train_validation(resnet_tuned_epochs, init_model_resnet_tuned, init_hyperparams_resnet_tuned, \n",
    "                         resnet_tuned_step, resnet_tuned_step_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fixed resnet model per-epoch losses & performance\n",
    "resnet_tuned_train_losses = resnet_tuned_losses_and_accs[0]\n",
    "resnet_tuned_train_accs = resnet_tuned_losses_and_accs[1]\n",
    "resnet_tuned_val_losses = resnet_tuned_losses_and_accs[2]\n",
    "resnet_tuned_val_accs = resnet_tuned_losses_and_accs[3]\n",
    "\n",
    "plot_results(resnet_tuned_train_losses, resnet_tuned_val_losses, resnet_tuned_epochs, model_type=\"Resnet-Tuned\", \\\n",
    "             result_type=\"Loss\")\n",
    "plot_results(resnet_tuned_train_accs, resnet_tuned_val_accs, resnet_tuned_epochs, model_type=\"Resnet-Tuned\", \\\n",
    "             result_type=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & test best model\n",
    "resnet_tuned_nn, resnet_tuned_optimizer = \\\n",
    "    best_model(resnet_tuned_models, resnet_tuned_losses, resnet_tuned_optimizers)\n",
    "test_model(resnet_tuned_nn, resnet_tuned_criterion, resnet_tuned_optimizer, resnet_tuned_step, \n",
    "           resnet_tuned_step_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and Feature Maps\n",
    "### Plotting & Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the given map in a square grid\n",
    "def plot_maps(data, data_type, title):\n",
    "    ncols = int(math.sqrt(data.size(0))) + 1\n",
    "    nrows = 1 + (data.size(0) - 1)//ncols\n",
    "\n",
    "    # figure width and height adjusted to # of plots\n",
    "    figwidth = ncols*0.6+(ncols-1)*0.1+0.6\n",
    "    figheight = nrows*0.6+(nrows-1)*0.1+0.6\n",
    "    top = (1 - 0.2/math.sqrt(figheight))\n",
    "\n",
    "    fig, plots = plt.subplots(nrows, ncols, figsize=(12,12))\n",
    "    fig.set_size_inches(figwidth,figheight)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.subplots_adjust(top=top)\n",
    "\n",
    "    for idx in range(data.size(0)):\n",
    "        if data_type == \"Weight\": # if weight map, get first channel only\n",
    "            datum = data[idx, 0].squeeze().cpu()\n",
    "        else: # if \"Feature\", get all channels\n",
    "            datum = data[idx].squeeze().cpu()\n",
    "            \n",
    "        r = idx // ncols\n",
    "        c = idx % ncols\n",
    "        plots[r,c].imshow(datum)\n",
    "        plots[r,c].axis('off')\n",
    "        plots[r,c].set_xticklabels([])\n",
    "        plots[r,c].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature maps for layers named in layersToPlot\n",
    "def get_feature_maps(model, layersToPlot, model_type):\n",
    "    first_test_data = test_dataset[0][0].unsqueeze(0).to(device)\n",
    "    \n",
    "    activation = {}\n",
    "    # hook according to pytorch documentation\n",
    "    def get_activation(name):\n",
    "        def hook(layer, m_input, m_output):\n",
    "            activation[name] = m_output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # get activation outputs\n",
    "    for name, layer in model._modules.items():\n",
    "        layer.register_forward_hook(get_activation(name))\n",
    "    output = model(first_test_data)\n",
    "\n",
    "    # plot feature maps for the given layers \n",
    "    for layer in layersToPlot:\n",
    "        act = activation[layer].squeeze() # \"layer output shape\" = size = [D2, W2, H2]\n",
    "        plot_maps(act, \"Feature\", model_type + \": Layer \" + layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight map of given layer\n",
    "def get_weight_maps(layer, model_type):\n",
    "    filters = layer.weight.detach() # \"filter shape\" = size = [64,3,3,3]\n",
    "    plot_maps(filters, \"Weight\", model_type + \" Weights\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom model feature & weight maps\n",
    "get_feature_maps(custom_nn, ['b1', 'b3', 'b6'], \"Custom\")\n",
    "get_weight_maps(custom_nn.b1[0], \"Custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fixed vgg model feature & weight maps\n",
    "get_feature_maps(vgg_fixed_nn.features, ['0', '14', '28'], \"VGG 16 (Fixed)\")\n",
    "get_weight_maps(vgg_fixed_nn.features[0], \"VGG 16 (Fixed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot finetuned vgg model feature & weight maps\n",
    "get_feature_maps(vgg_tuned_nn.features, ['0', '14', '28'], \"VGG 16 (Finetuned)\")\n",
    "get_weight_maps(vgg_tuned_nn.features[0], \"VGG 16 (Finetuned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fixed resnet model feature & weight maps\n",
    "get_feature_maps(resnet_fixed_nn, ['conv1', 'layer2', 'layer4'], \"ResNet 18 (Fixed)\")\n",
    "get_weight_maps(resnet_fixed_nn.conv1, \"ResNet 18 (Fixed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot finetuned resnet model feature & weight maps\n",
    "get_feature_maps(resnet_tuned_nn, ['conv1', 'layer2', 'layer4'], \"ResNet 18 (Finetuned)\")\n",
    "get_weight_maps(resnet_tuned_nn.conv1, \"ResNet 18 (Finetuned)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
